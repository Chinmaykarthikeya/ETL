{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db46e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 0. LOGGING CONFIGURATION\n",
    "# ---------------------------------------------------------\n",
    "# This sets up the logger to write to a file AND print to the console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"etl_process.log\"), \n",
    "        logging.StreamHandler(sys.stdout)       \n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1bab00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl_pipeline():\n",
    "    \n",
    "    logger.info(\">>> ETL Process Started\")\n",
    "    sales_dir = 'C:\\\\Users\\\\karth\\\\Downloads\\\\Python Assignment\\\\Python Assignment\\\\Sales_Split'\n",
    "    # ---------------------------------------------------------\n",
    "    # Checking File Path\n",
    "    # ---------------------------------------------------------\n",
    "    try:\n",
    "        if not os.path.exists(sales_dir):\n",
    "            logger.error(f\">>> Critical Error: Directory '{sales_dir}' not found.\")\n",
    "            return\n",
    "        file_list = [f for f in os.listdir(sales_dir) if f.endswith('.csv')]\n",
    "        file_count = len(file_list)\n",
    "        logger.info(f\">>> Scanning directory Ended.\")\n",
    "        logger.info(f\">>> Found {file_count} sales files to process.\")\n",
    "        if file_count == 0:\n",
    "            logger.warning(\">>> No CSV files found in the directory. Stopping process.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\">>> Error accessing directory '{sales_dir}': {e}\")\n",
    "        return\n",
    "    # ---------------------------------------------------------\n",
    "    # Loading and Merging the datasets\n",
    "    # ---------------------------------------------------------\n",
    "    try:\n",
    "        data_frames = []\n",
    "        \n",
    "        logger.info(\">>> Reading files ...\")\n",
    "        for filename in file_list:\n",
    "            file_path = os.path.join(sales_dir, filename)\n",
    "            try:\n",
    "                # Read individual file\n",
    "                df = pd.read_csv(file_path)\n",
    "                data_frames.append(df)\n",
    "            except Exception as e:\n",
    "                logger.error(f\">>> Failed to read file {filename}: {e}\")\n",
    "                continue\n",
    "        if not data_frames:\n",
    "            logger.error(\">>> No valid dataframes loaded.\")\n",
    "            return\n",
    "        all_sales_data = pd.concat(data_frames, ignore_index=True)\n",
    "        total_rows = len(all_sales_data)\n",
    "        logger.info(f\">>> Successfully merged {len(data_frames)} files. Total sales records: {total_rows}\")\n",
    "        \n",
    "    \n",
    "        if total_rows > 0:\n",
    "            logger.info(\">>> Validation Passed: Sales data is not empty. Proceeding to Joins.\")\n",
    "        try:\n",
    "                store_master = pd.read_csv('../store_master.csv')\n",
    "                product_master = pd.read_csv('../product_master.csv')\n",
    "                logger.info(f'>>> Total Rows in Store_Master Dataset {len(store_master)}')\n",
    "                logger.info(f'>>> Total Rows in Product_Master Dataset {len(product_master)}')\n",
    "        except FileNotFoundError as e:\n",
    "                logger.error(f\">>> Critical Error: Master file missing - {e}\")\n",
    "                return\n",
    "        \n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "            # Left Joing the three datasets Extracted.\n",
    "            # -----------------------------------------------------\n",
    "        merged_df = pd.merge(all_sales_data, store_master, on='store_id', how='left')\n",
    "        logger.info('>>> Joined Sales Data with Store Dataset')\n",
    "        merged_df = pd.merge(merged_df, product_master, on='sku', how='left')\n",
    "        logger.info(\">>> Joined Sales Data with Product Dataset.\")\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "            # 5.Filtering for the store with active status column\n",
    "            # -----------------------------------------------------\n",
    "        if 'status' in merged_df.columns:\n",
    "            active_sales_df = merged_df[merged_df['status'] == 'Active'].copy()\n",
    "            logger.info(\">>> Filtered for Active stores.\")\n",
    "        else:\n",
    "            logger.warning(\">>> Warning: 'status' column not found. Skipping filter.\")\n",
    "            active_sales_df = merged_df\n",
    "        # -----------------------------------------------------\n",
    "            # 6. Aggreating the Data with  sum() function\n",
    "            # -----------------------------------------------------\n",
    "        logger.info(\">>> Aggregating data...\")\n",
    "        agg_list=['store_id', 'electronics_type', 'classification']\n",
    "        name_qunty=['sales_qty', 'sales_value']\n",
    "        aggregated_df = active_sales_df.groupby(\n",
    "                agg_list\n",
    "            )[name_qunty].sum().reset_index()\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "            # Saving the Aggregated data\n",
    "            # -----------------------------------------------------\n",
    "        output_file = 'aggregated_sales.csv'\n",
    "        aggregated_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\">>> SUCCESS. Final output saved as '{output_file}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\">>> Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b823181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:47:24,622 - INFO - >>> ETL Process Started\n",
      "2025-12-22 14:47:24,626 - INFO - >>> Scanning directory Ended.\n",
      "2025-12-22 14:47:24,628 - INFO - >>> Found 20 sales files to process.\n",
      "2025-12-22 14:47:24,629 - INFO - >>> Reading files ...\n",
      "2025-12-22 14:47:24,626 - INFO - >>> Scanning directory Ended.\n",
      "2025-12-22 14:47:24,628 - INFO - >>> Found 20 sales files to process.\n",
      "2025-12-22 14:47:24,629 - INFO - >>> Reading files ...\n",
      "2025-12-22 14:47:37,811 - INFO - >>> Successfully merged 20 files. Total sales records: 20000000\n",
      "2025-12-22 14:47:37,813 - INFO - >>> Validation Passed: Sales data is not empty. Proceeding to Joins.\n",
      "2025-12-22 14:47:37,811 - INFO - >>> Successfully merged 20 files. Total sales records: 20000000\n",
      "2025-12-22 14:47:37,813 - INFO - >>> Validation Passed: Sales data is not empty. Proceeding to Joins.\n",
      "2025-12-22 14:47:37,914 - INFO - >>> Total Rows in Store_Master Dataset 20000\n",
      "2025-12-22 14:47:37,915 - INFO - >>> Total Rows in Product_Master Dataset 100000\n",
      "2025-12-22 14:47:37,914 - INFO - >>> Total Rows in Store_Master Dataset 20000\n",
      "2025-12-22 14:47:37,915 - INFO - >>> Total Rows in Product_Master Dataset 100000\n",
      "2025-12-22 14:47:47,148 - INFO - >>> Joined Sales Data with Store Dataset\n",
      "2025-12-22 14:47:47,148 - INFO - >>> Joined Sales Data with Store Dataset\n",
      "2025-12-22 14:48:05,927 - INFO - >>> Joined Sales Data with Product Dataset.\n",
      "2025-12-22 14:48:05,927 - INFO - >>> Joined Sales Data with Product Dataset.\n",
      "2025-12-22 14:48:20,606 - INFO - >>> Filtered for Active stores.\n",
      "2025-12-22 14:48:20,638 - INFO - >>> Aggregating data...\n",
      "2025-12-22 14:48:20,606 - INFO - >>> Filtered for Active stores.\n",
      "2025-12-22 14:48:20,638 - INFO - >>> Aggregating data...\n",
      "2025-12-22 14:48:33,091 - INFO - >>> SUCCESS. Final output saved as 'aggregated_sales.csv'.\n",
      "2025-12-22 14:48:33,091 - INFO - >>> SUCCESS. Final output saved as 'aggregated_sales.csv'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_etl_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a3a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
